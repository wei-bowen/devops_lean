## 卷挂载
**可用的卷类型**
- emptyDir： 用于存储临时数据的简单空目录
- hostPath: 将节点上的文件系统挂载到Pod中
- gitRepo: 其实也是个emptyDir，启动的时候从git仓库拉取内容
- nfs: nfs共享目录，挂载类似hostPath
- 各类云盘
- configMap、secret、downwardAPI: 用于将k8s部分资源和集群信息公开给Pod的特殊类型的卷(下一章节讲述)
- persistentVolumeClain: 一种使用预置或者动态配置的持久存储类型

### 挂载普通券(空白、本地、nfs等其他共享磁盘)

>**挂载空白临时卷**
```yaml
##spec下新增卷volumes，然后再containers下添加挂载volumeMounts
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    name: fortune
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: webserver
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true                      ##不指定时默认可读写
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}

### 可以指定空白券在内存中创建而非硬盘。示例：
#   emptyDir:
#     medium: Memory
```
>**添加giterepo挂载**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: webserver1
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/wei-bowen/kubia-website-example.git
      revision: master
      directory: .      ### .代表将git-repo拉取到空白券的根目录下
```
>**添加节点本地文件夹hostpath挂载**
```yaml
### 1、分享节点上的一个目录给pod内的容器使用.  可以数据存储在节点上，pod生命周期结束后不丢失。缺点时pod分布在不同节点而且随时可能变换，原来的数据将无法继续绑定
apiVersion: v1
kind: Pod
metadata:
  name: hostpath
spec:
  containers:
  - image: luksa/fortune
    name: fortune
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: webserver
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    hostPath:
      path: /tmp/html
      type: DirectoryOrCreate   

### type可选项
#   DirectoryOrCreate 目录不存在自动创建
#   Directory 目录必须存在
#   FileOrCreate 文件不存在则创建空白文件
#   File 文件必须存在
```
>**添加nfs等其他类型网盘挂载**
需求类似Mysql数据库，需要将数据存储到磁盘上，Pod重新调度到其他节点上也要访问跟原来一样的数据。所以底层存储必须是持久的可共享的网络磁盘。此处以NFS存盘为例
- 创建NFS共享目录（可以在专门的文件服务器，此处选用了k8s-master，192.168.0.77）
```
yum install -y  nfs-utils rpcbind
systemctl start rpcbind && systemctl enable rpcbind
systemctl start nfs && systemctl enable nfs
echo "/data/nfs_share 192.168.0.0/24(rw,sync,no_root_squash) 10.244.0.0/24(rw,sync,no_root_squash)" > /etc/exports  ##允许两个网段共享，一个是节点机器IP段，另一个是集群网络
exportfs -r           ##每次修改exports配置后都要刷新
## 节点机执行 yum install nfs-utils 即可挂载共享目录 
showmount -e 192.168.0.77                        ##即可看到共享的目录
mount -t nfs 192.168.0.77:/data/nfs_share /nfs  ##将远程nfs目录挂载到指定目录下
mount
```
- 将目录挂载到Pod下
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfspath
spec:
  containers:
  - image: luksa/fortune
    name: fortune
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: webserver
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    nfs:
      server: 192.168.0.77              ## 所有node节点都需要安装 nfs客户端 nfs-utiles
      path: /data/nfs_share

>**其他可选共享磁盘**
#   gcePersitentDisk:       谷歌GCE存储
#     pdName: mongodb       磁盘ID
#     fsType: ext4          文件系统类型
#   
#   awsElasticBlockStore        AWS存储
#     volumeId: my-volume       
#     fsType: ext4
```
### 持久券和持久券声明(从底层存储技术解耦Pod)
开发人员不需要关心底层存储的架构信息，当他们需要持久化存储时可以直接向k8s请求。 <br>
![持久卷应用](https://github.com/wei-bowen/devops_lean/blob/main/images/pv-pvc.jpg.jpeg)
- PV: PersistenVolume持久卷。集群级别的资源，可以被任何命名空间使用。
- PVC： PersistentVolumeClaim持久卷声明
>**基于NFS向K8S申明一个PV**
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongdb-nfs-pv
spec:
  accessModes:                                ##访问模式，可多选
  - ReadWriteOnce                             ##允许单用户读写模式引用
  - ReadOnlyMany                              ##允许多用户只读模式引用
  capacity:
    storage: 1Gi                              ##大小为1G
  nfs:
    server: 192.168.0.77              
    path: /data/nfs_share
  persistentVolumeReclaimPolicy: Retain       ##声明释放后，PV仍保留
```
创建成功。
```
[root@k8s-master ~]# kubectl apply -f mongdb-nfs-pv.yaml
[root@k8s-master ~]# kubectl get pv -o wide
NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE     VOLUMEMODE
mongdb-nfs-pv   1Gi        RWO,ROX        Retain           Available                                   5m49s   Filesystem
```
- PV属于集群层面的资源，不属于任何命名空间
- ACCESS MODES： RWO允许单节点挂载读写，ROX允许多节点挂载只读，RWX允许多节点挂载读写
- RECLAIM POLICY： PVC释放后的PV保留策略，retain时PVC释放后PV保留但是不可被新PVC绑定(数据留存在存储上)，Recycle时PV保留可被使用但是数据被清除，Delete删除底层存储
>**创建PVC来获取PV**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongdb-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ""
##PVC会根据大小和挂载模式要求自动查找匹配VC
```
创建成功
```
[root@k8s-master ~]# kubectl get pvc -o wide
NAME         STATUS   VOLUME          CAPACITY   ACCESS MODES   STORAGECLASS   AGE    VOLUMEMODE
mongdb-pvc   Bound    mongdb-nfs-pv   1Gi        RWO,ROX                       101s   Filesystem
You have new mail in /var/spool/mail/root
[root@k8s-master ~]# kubectl get pv -o wide
NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE   VOLUMEMODE
mongdb-nfs-pv   1Gi        RWO,ROX        Retain           Bound    default/mongdb-pvc                           19m   Filesystem
```
- STATUS： 可以看到PV的挂载状态由available变成了bound ，可用变成已绑定
- CLAIM：  绑定的PVC，显示已自动绑定到default命名空间下的mongdb-pvc
>**在Pod中使用PVC**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
  labels:
    app: db
spec:
  containers:
  - image: mongo
    name: mongodb
    ports:
    - containerPort: 27017
      protocol: TCP
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongdb-pvc
```
创建成功后即可在nfs目录下看到mongodb初始化的文件
```
[root@k8s-master ~]# ll /data/nfs_share/
total 12
drwx------ 2 polkitd input   41 Sep 21 15:47 journal
-rw------- 1 polkitd input    0 Sep 21 15:47 mongod.lock
-rw------- 1 polkitd input   47 Sep 21 15:47 WiredTiger
-rw------- 1 polkitd input    0 Sep 21 15:47 WiredTiger.lock
-rw------- 1 polkitd input  963 Sep 21 15:47 WiredTiger.turtle
-rw------- 1 polkitd input 4096 Sep 21 15:47 WiredTiger.wt
```
>**回收持久卷**
- 删除Pod和PVC，重新声明PVC，原来的模式为Retain的PV不会自动绑定到全新的PVC上，因为PV还包含了旧数据。

**-----------------------------持久卷的动态卷配置----------------------------------------**
通过置备程序provisioner(k8s部署在本地时需要安装，一般云服务器供应商会提供)，管理员只需要定义一个StorageClass允许系统在每次通过持久卷声明请求时自动创建一个新的持久卷。<br>
如果现有的存储盘耗尽，还能自动购买云盘。
云端的操作可以参考[阿里云有状态服务-动态云盘使用最佳实践](https://help.aliyun.com/document_detail/100457.html?spm=a2c4g.11186623.6.1102.5a1825fembFlih)
- 1、定义StorageClass资源定义可用存储类型
```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: alicloud-disk-ssd-hangzhou-b
provisioner: alicloud/disk                      ##指定provisioner置备程序
reclaimPolicy: Retain                           ##云盘的回收策略，默认Delete，盘加数据一起删除，不可恢复
parameters:
  type: cloud_ssd                               ##类型
  regionid: cn-hangzhou                         ##资源中心，云盘所属资源中心需要与集群一致
  zoneid: cn-hangzhou-b                         ##可用区中心，云盘所属资源中心需要与集群一致
  fstype: "ext4"                                ##挂载的文件系统类型
  readonly: "false"                             ##默认可读写
  encrypted： "false"                           ##默认不加密
```
- 2、创建pv
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongdb-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: "alicloud-disk-ssd-hangzhou-b"          ##与上面一致，除了此处声明了需要引用的StorageClass。创建成功后会自动购买云盘声明到PV并绑定PVC
```
>**本地K8s集群的持久卷的动态卷配置**
- 1、安装provisioner置备程序. 此处引用 [kubernetes持久化存储-StorageClass](https://www.jianshu.com/p/aca71580a7c8)
```yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 192.168.0.77
            - name: NFS_PATH
              value: /data/nfs_share
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.0.77
            path: /data/nfs_share
```
....哎。涉及大盘加密账号，认证等问题，还没学到，先跳过了
